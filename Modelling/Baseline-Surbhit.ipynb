{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12731519,"sourceType":"datasetVersion","datasetId":7917949}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## BASELINE MODELLING","metadata":{}},{"cell_type":"code","source":"!pip install tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import Counter\ntqdm.pandas()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:07:43.518986Z","iopub.execute_input":"2025-08-19T03:07:43.519254Z","iopub.status.idle":"2025-08-19T03:07:43.812370Z","shell.execute_reply.started":"2025-08-19T03:07:43.519230Z","shell.execute_reply":"2025-08-19T03:07:43.811547Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/santander-recsys/sample_submission.csv\n/kaggle/input/santander-recsys/first_acquisition_df.csv\n/kaggle/input/santander-recsys/test_ver2.csv\n/kaggle/input/santander-recsys/train_ver2.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Load the datasets","metadata":{}},{"cell_type":"code","source":"test_path = \"/kaggle/input/santander-recsys/test_ver2.csv\"\ntrain_path = \"/kaggle/input/santander-recsys/train_ver2.csv\"\nsample_path = \"/kaggle/input/santander-recsys/sample_submission.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:07:43.814163Z","iopub.execute_input":"2025-08-19T03:07:43.814593Z","iopub.status.idle":"2025-08-19T03:07:43.818336Z","shell.execute_reply.started":"2025-08-19T03:07:43.814574Z","shell.execute_reply":"2025-08-19T03:07:43.817543Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data = pd.read_csv(train_path)\nprint(f\"Training dataset loaded. Shape : {train_data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:07:43.839428Z","iopub.execute_input":"2025-08-19T03:07:43.840100Z","iopub.status.idle":"2025-08-19T03:09:03.073488Z","shell.execute_reply.started":"2025-08-19T03:07:43.840076Z","shell.execute_reply":"2025-08-19T03:09:03.072825Z"}},"outputs":[{"name":"stdout","text":"Training dataset loaded. Shape : (13647309, 48)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"test_data = pd.read_csv(test_path)\nprint(f\"Testing dataset loaded. Shape : {test_data.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_data = pd.read_csv(sample_path)\nprint(f\"Sample dataset loaded. Shape : {sample_data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T02:17:29.959802Z","iopub.execute_input":"2025-08-19T02:17:29.960025Z","iopub.status.idle":"2025-08-19T02:17:30.521485Z","shell.execute_reply.started":"2025-08-19T02:17:29.960007Z","shell.execute_reply":"2025-08-19T02:17:30.520671Z"}},"outputs":[{"name":"stdout","text":"Sample dataset loaded. Shape : (929615, 2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def get_all_products():\n    product_cols = [col for col in train_data.columns if col.startswith('ind_') and col.endswith('_ult1')]  # Adjust to actual product column names\n    return product_cols\n    print(f\"Total products : {len(product_cols)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:09:42.474034Z","iopub.execute_input":"2025-08-19T03:09:42.474310Z","iopub.status.idle":"2025-08-19T03:09:42.478483Z","shell.execute_reply.started":"2025-08-19T03:09:42.474287Z","shell.execute_reply":"2025-08-19T03:09:42.477797Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Evaluation function - MAP@7","metadata":{}},{"cell_type":"code","source":"def apk(actual, predicted, k=7):\n    if len(predicted) > k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\ndef mapk(actual_list, predicted_list, k=7):\n    return np.mean([apk(a, p, k) for a, p in zip(actual_list, predicted_list)])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline 1 - Popularity based recommendation\n\n- Recommending the most popular products (7) to the users","metadata":{}},{"cell_type":"markdown","source":"### Splitting dataset into train and eval\n\n\n1. The cutoff date (`training_cutoff`) for splitting is set as 2016-04-28\n2. Data before `training_cutoff` is taken as training data and data after this is eval data","metadata":{}},{"cell_type":"code","source":"training_cutoff = train_data['fecha_dato'].sort_values().unique()[-2]\n\ntrain_df = train_data[train_data['fecha_dato'] <= training_cutoff]\neval_df = train_data[train_data['fecha_dato'] > training_cutoff]\n\nprint(f\"Cutoff date : {training_cutoff}\")\nprint(f\"Train data shape : {train_df.shape}\")\nprint(f\"Eval data shape : {eval_df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding most popular products\n\n1. For all the customers, find the product which have been ever owned (1 if owned atleast once, else 0)\n2. Sum it for all the products across each customer, this will give us the most popular products","metadata":{}},{"cell_type":"code","source":"product_cols = get_all_products()\nprint(f\"Total products : {len(product_cols)}\\n\")\ncolumns_to_use = ['ncodpers'] + product_cols\n\ntrain_df_subset = train_df[columns_to_use]\nprint(f\"Training data shape after selecting product columns and customer id : {train_df_subset.shape}\\n\")\n\n# 1. Aggregate to ever-owned per user (1 if ever owned in history, else 0)\never_owned_products = train_df_subset.groupby('ncodpers')[product_cols].max()\n\n# 2. Sum columns to get total number of customers who ever owned each product\nproduct_popularity = ever_owned_products.sum(axis=0).sort_values(ascending=False)\ntop_7_products = product_popularity.nlargest(7).index.tolist()\nprint(f\"Top 7 Product: \\n{', '.join(top_7_products)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### `get_owned_and_popular_products()`\n\n- This function returns the products owned atleast once by the customer (`ever_owned_products`) and the most popular products (`top_products`)","metadata":{}},{"cell_type":"code","source":"def get_owned_and_popular_products():\n    columns_to_use = ['ncodpers'] + product_cols\n    train_df = train_data[columns_to_use]\n    ever_owned_products = train_df.groupby('ncodpers')[product_cols].max()\n    product_popularity = ever_owned_products.sum(axis=0).sort_values(ascending=False)\n    top_products = product_popularity.index.tolist()\n    return ever_owned_products, top_products","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### `recommend_for_user(user_id, top_n=7)`\n\n- Takes a customer id as input and return the top products never owned by the customer as the recommendation (default `top_n` = 7)","metadata":{}},{"cell_type":"code","source":"def recommend_for_user(user_id, top_n=7):\n    owned_products = user_owned[user_id]\n    recs = [p for p in top_products if p not in owned_products][:top_n]\n    recs = ' '.join(recs)\n    return recs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Building a dictionary of products each user has ever owned\n\n- Loops over all users in `ever_owned_products` and creates a dictionary where each `uid` maps to a **set of products** they have ever owned (i.e., products marked with `1`).","metadata":{}},{"cell_type":"code","source":"ever_owned_products, top_products = get_owned_and_popular_products()\ntop_products = product_popularity.index.tolist()\nuser_owned = {\n    uid: set(ever_owned_products.loc[uid][ever_owned_products.loc[uid] == 1].index)\n    for uid in ever_owned_products.index\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Baseline predictions on evaluation set\n\n- Blindly predicting the top 7 recommendation from most popular products","metadata":{}},{"cell_type":"code","source":"# Get unique customers in May 2016\neval_users = eval_df['ncodpers'].unique()\nprint(f\"total users in eval set: {len(eval_users)}\")\n\n# Predict same top 7 for all\nbaseline_eval_preds = pd.DataFrame({\n    'ncodpers': eval_users,\n    'added_products': [top_7_products] * len(eval_users)\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Eval Ground Truth\n\n* Constructing the ground truth product ownership for evaluation users.\n* `eval_truth` is a list of lists, where each inner list contains the products owned by a customer as per `eval_df`.","metadata":{}},{"cell_type":"code","source":"# Ground truth from eval_df\neval_truth = []\nfor _, row in eval_df.groupby('ncodpers'):\n    added = [prod for prod in product_cols if row[prod].values[0] == 1]\n    eval_truth.append(added)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Computing MAP@7 for baseline evaluation data","metadata":{}},{"cell_type":"code","source":"map_score_eval = mapk(eval_truth, baseline_eval_preds['added_products'].tolist())\nprint(f\"Baseline 1 MAP@7 (Eval data): {map_score_eval:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test Set Recommendations\n- For each user in the test set, generating recommendation using `recommend_for_user()`","metadata":{}},{"cell_type":"code","source":"sample_data['added_products'] = sample_data['ncodpers'].map(recommend_for_user)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Error Analysis\n\n- We have evaluation ground truth data `eval_truth` and evaluation predictions `eval_preds`\n- Using these data, we will compute the **False Positives** and **False Negatives**","metadata":{}},{"cell_type":"code","source":"fp_counter,fn_counter = Counter(), Counter()\n\neval_preds = baseline_eval_preds['added_products'].tolist()\n\nfor true_prod, pred_prod in zip(eval_truth, eval_preds):\n    true_set = set(true_prod)\n    pred_set = set(pred_prod)\n\n    # False Negatives: missed actual buys\n    fn = true_set - pred_set\n    fn_counter.update(fn)\n\n    # False Positives: predicted but not actually bought\n    fp = pred_set - true_set\n    fp_counter.update(fp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Count of Top 10 False Negatives","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 false negatives (bought but not predicted):\")\nfor product, count in fn_counter.most_common(10):\n    print(f\"{product} : {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Count of Top 7 False Positives","metadata":{}},{"cell_type":"code","source":"# Since we've predicted 7 fixed items per user\nprint(\"Top 7 false positives (predicted but not bought):\")\nfor product, count in fp_counter.most_common():\n    print(f\"{product} : {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_data.to_csv(\"Baseline1_test_preds.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline 2 - Popularity Based on Segments\n\n* Customer attributes such as `age`, `nomprov`, `segmento`, and `renta` are binned into discrete buckets.\n* These buckets define customer segments, and recommendations are made by suggesting the most popular products within the customer’s segment.","metadata":{}},{"cell_type":"markdown","source":"### Basic data preprcessing steps: \n1. Replace '   NA' in `age` with np.nan\n2. Convert age to numeric type\n3. Find ids where all 3 - `renta`, `nomprov` and `age` are nan\n4. Drop records belongning to above ids","metadata":{}},{"cell_type":"code","source":"# Generic preprocessing steps: \n\n# 1. Replace ' NA' in age with np.nan\ntrain_data['age'] = train_data['age'].replace(' NA', np.nan)\n\n# 2. Convert age column to numeric type\ntrain_data['age'] = pd.to_numeric(train_data['age'], errors='coerce')\n\n# 3. Select customer IDs with all 3 key fields missing (renta, nomprov, age)\nids_with_nan =  train_data[train_data['renta'].isna() & \n                train_data['nomprov'].isna() & \n                train_data['age'].isna()]['ncodpers']\nprint(f\"Total customers with no age, renta and nomprov record : {len(set(ids_with_nan))}\")\n\n# 4. Dropping customers with no age, renta and nomprov record\ndf_train = train_data[~(train_data['age'].isna() & train_data['nomprov'].isna() & train_data['renta'].isna())]\nprint(f\"Shape of data after removing customers with no age, nomprov and renta : {df_train.shape}\")\n\n\n# Sanity Check - Number of records after dropping should be equal to total records - len(ids_with_nan)\nassert df_train.shape[0] == (train_data.shape[0] - len(ids_with_nan))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculating 33rd and 67th percentile of renta for bucketing","metadata":{}},{"cell_type":"code","source":"q_33 = df_train['renta'].quantile(0.33)\nprint(f\"33rd percentile of renta : {q_33}\")\nq_67 = df_train['renta'].quantile(0.67)\nprint(f\"67th percentile of renta : {q_67}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Functions to bucketize the values","metadata":{}},{"cell_type":"code","source":"# Function to bucketize age into ranges\ndef age_bucket(age):\n    if pd.isna(age):          # If age is missing, return \"UNK\"\n        return \"UNK\"\n    elif age < 18:            # Below 18\n        return \"0-17\"\n    elif age < 30:            # 18 to 29\n        return \"18-29\"\n    elif age < 45:            # 30 to 44\n        return \"30-44\"\n    elif age < 60:            # 45 to 59\n        return \"45-59\"\n    else:                     # 60 and above\n        return \"60+\"\n\n# Function to bucketize income into categories\ndef income_bucket(income):\n    if pd.isna(income):       # If income is missing\n        return \"UNK\"\n    elif income <= 80000:     # Low income\n        return \"L\"\n    elif income <= 133700:    # Medium income\n        return \"M\"\n    else:                     # High income\n        return \"H\"\n\n# Function to bucketize location (province)\ndef nomprov_bucket(loc):\n    if pd.isna(loc):          # If location is missing\n        return \"UNK\"\n    else:                     # Otherwise, return the location itself\n        return loc\n\n# Function to bucketize gender\ndef sexo_bucket(gdr):\n    if pd.isna(gdr):          # If gender is missing\n        return \"UNK\"\n    else:                     # Otherwise, return gender itself\n        return gdr\n\n# Function to bucketize customer segment\ndef segmento_bucket(seg):\n    if pd.isna(seg):          # If segment is missing\n        return \"UNK\"\n        \n    if seg == \"01 - TOP\":     # Top segment\n        return \"TOP\"\n        \n    if seg == \"02 - PARTICULARES\":  # Individual customers\n        return \"PARTICULARES\"\n        \n    if seg == \"03 - UNIVERSITARIO\": # University students\n        return \"UNIVERSITARIO\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performing bucketzation and creating segments\n1. `bucket_functions` is a dict which contain features to consider for bucketing and the corresponding function to implement\n2. Apply bucketization for all features and store it in `<feature>_bucket` column\n3. Create segments in format `<age_bucket>_<nomprov_bucket>_<renta_bucket>_<segmento_bucket>`","metadata":{}},{"cell_type":"code","source":"# Mapping of columns to their respective bucketization functions\nbucket_functions = {\n    'age': age_bucket,\n    'renta': income_bucket,\n    'nomprov': nomprov_bucket,\n    'segmento': segmento_bucket,\n    'sexo': sexo_bucket\n}\n\n# Apply bucketization for each column\nfor col, func in bucket_functions.items():\n    df_train[f'{col}_bucket'] = df_train[col].apply(func)\n\n\ndf_train['segment'] = (\n    df_train['age_bucket'] + \"_\" +\n    df_train['nomprov_bucket'].astype(str) + \"_\" +\n    df_train['renta_bucket'] + \"_\" +\n    df_train['segmento_bucket'].astype(str)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting dataset into train and eval\n\n\n1. The cutoff date (`training_cutoff`) for splitting is set as 2016-04-28\n2. Data before `training_cutoff` is taken as training data and data after this is eval data","metadata":{}},{"cell_type":"code","source":"product_cols = get_all_products()\n\n# Splitting into train and eval set. The last month's data is kept as evaluation dataset\n\ntraining_cutoff = df_train['fecha_dato'].sort_values().unique()[-2]\n\ntrain_df = df_train[df_train['fecha_dato'] <= training_cutoff]\neval_df = df_train[df_train['fecha_dato'] > training_cutoff]\n\nprint(f\"Cutoff date : {training_cutoff}\")\nprint(f\"Data before {training_cutoff} is for training and data after is for evaluation\")\nprint(f\"Train data shape : {train_df.shape}\")\nprint(f\"Eval data shape : {eval_df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding most popular products\n\n1. For all the customers, find the product which have been ever owned (1 if owned atleast once, else 0) (`ever_owned`)\n2. Get the corresponding segment from `df_train` and merge both dataframes\n3. Group the merged dataframe `ever_owned` based on `segment` and find top products for each segment","metadata":{}},{"cell_type":"code","source":"ever_owned = train_df.groupby('ncodpers')[product_cols].max()\n\nsegment_info = df_train[['ncodpers', 'segment']].drop_duplicates(subset='ncodpers').set_index('ncodpers')\never_owned = ever_owned.merge(segment_info, left_index=True, right_index=True)\n\n\nsegment_popularity = ever_owned.groupby('segment')[product_cols].sum()\n\n# Storing the top products for each segment in segment_top_products\nsegment_top_products = {\n    segment: segment_popularity.loc[segment].sort_values(ascending=False).index.tolist()\n    for segment in segment_popularity.index\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top Products Across All Customers\n\n* `top_products` stores the most frequently owned products across the entire data\n* Acts as a fallback recommendation list for cases where a customer ID is missing or not found during evaluation","metadata":{}},{"cell_type":"code","source":"# Finding top products across all users. This will be used for cases if userid is not present in the ever_owned dict\n\ncolumns_to_use = ['ncodpers'] + product_cols\ntrain_df_gbl = train_df[columns_to_use]\never_owned_products = train_df_gbl.groupby('ncodpers')[product_cols].max()\nproduct_popularity = ever_owned_products.sum(axis=0).sort_values(ascending=False)\ntop_products = product_popularity.index.tolist()\ndel train_df_gbl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Recommending on evaluation data\n- `baseline2_recommend_for_user(..)` - Takes the user id and returns top product recommendations which have not been owned by user.","metadata":{}},{"cell_type":"code","source":"def baseline2_recommend_for_user(user_id, top_n=7):\n    # Case 1: If user is not in the dataset, return global top products\n    if user_id not in ever_owned.index:\n        return top_products[:top_n]\n        \n    # Case 2: If user exists, recommend based on their segment       \n    user_data = ever_owned.loc[user_id]\n    \n    # Identify the segment the user belongs to\n    user_segment = user_data['segment']\n    \n    # Get the set of products already owned by the user\n    owned_products = set(user_data[product_cols][user_data[product_cols] == 1].index)\n    \n    # Fetch popular products for this segment\n    popular_products = segment_top_products.get(user_segment, [])\n\n    # Filter out products the user already owns\n    recs = [p for p in popular_products if p not in owned_products]\n\n    # Return top 7 recommendations\n    return recs[:top_n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Predictions","metadata":{}},{"cell_type":"code","source":"# Eval predictions\n\n# Get unique customers in May 2016\neval_users = eval_df['ncodpers'].unique()\nprint(f\"total users in eval set: {len(eval_users)}\")\n\nbaseline_eval_preds = pd.DataFrame({\n    \"ncodpers\": eval_users\n})\n\nbaseline_eval_preds[\"added_products\"] = baseline_eval_preds[\"ncodpers\"].map(lambda uid: baseline2_recommend_for_user(uid, top_n=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ground truth for evaluation predictions","metadata":{}},{"cell_type":"code","source":"# Ground truth from eval_df\neval_actual = []\nfor _, row in eval_df.groupby('ncodpers'):\n    added = [prod for prod in product_cols if row[prod].values[0] == 1]\n    eval_actual.append(added)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Computing MAP@7 for baseline evaluation data","metadata":{}},{"cell_type":"code","source":"map_score_eval = mapk(eval_actual, baseline_eval_preds['added_products'].tolist())\nprint(f\"Baseline 2 MAP@7 (Eval data): {map_score_eval:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Error Analysis\n\n- We have evaluation ground truth data `eval_actual` and evaluation predictions `eval_preds`\n- Using these data, we will compute the **False Positives** and **False Negatives**","metadata":{}},{"cell_type":"code","source":"fp_counter,fn_counter = Counter(), Counter()\n\neval_preds = baseline_eval_preds['added_products'].tolist()\n\nfor true_prod, pred_prod in zip(eval_actual, eval_preds):\n    true_set = set(true_prod)\n    pred_set = set(pred_prod)\n\n    # False Negatives: missed actual buys\n    fn = true_set - pred_set\n    fn_counter.update(fn)\n\n    # False Positives: predicted but not actually bought\n    fp = pred_set - true_set\n    fp_counter.update(fp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top 10 False Negatives","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 false negatives (bought but not predicted):\")\nfor product, count in fn_counter.most_common(10):\n    print(f\"{product} : {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top 10 False Positives","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 false positives (predicted but not bought):\")\nfor product, count in fp_counter.most_common(10):\n    print(f\"{product} : {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Recommendation on the Test Set","metadata":{}},{"cell_type":"code","source":"# 5. Define recommendation function per user for test set\n\never_owned = df_train.groupby('ncodpers')[product_cols].max()\nsegment_info = df_train[['ncodpers', 'segment']].drop_duplicates(subset='ncodpers').set_index('ncodpers')\never_owned = ever_owned.merge(segment_info, left_index=True, right_index=True)\n\ndef recommend_for_user(user_id, top_n=7):\n    if user_id not in ever_owned.index:\n        print(f\"User not found: {user_id}\")\n        return []\n    user_data = ever_owned.loc[user_id]\n    user_segment = user_data['segment']\n    owned_products = set(user_data[product_cols][user_data[product_cols] == 1].index)\n    popular_products = segment_top_products.get(user_segment, [])\n    recs = [p for p in popular_products if p not in owned_products]\n    return recs[:top_n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Batch Prediction\n\n- Implemented to prevent kernel crash","metadata":{}},{"cell_type":"code","source":"batch_size = 400000\nn = len(sample_data)\nall_recs = []\n\nfor start in range(0, n, batch_size):\n    end = start + batch_size\n    print(f\"Recommending for {start} to {end}\")\n    batch = sample_data.iloc[start:end]\n    recs = batch['ncodpers'].progress_apply(lambda uid: \" \".join(recommend_for_user(uid, top_n=5)))\n    all_recs.extend(recs)\n\nsample_data['added_products'] = all_recs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_data.to_csv(\"Baseline2_test_preds.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline 3 - Popularity Based on Segments\n\n* Customer attributes such as `age`, `nomprov`, `segmento`, `renta` and `antiguedad` are binned into discrete buckets.\n* These buckets define customer segments, and recommendations are made by suggesting the most popular products within the customer’s segment.","metadata":{}},{"cell_type":"markdown","source":"### Basic data preprcessing steps: \n1. Replace '   NA' in `age` with np.nan\n2. Replace '   NA' in `antiguedad` with np.nan, trim spaces and convert to numeric type\n3. Find ids where all 5 - `renta`, `nomprov`, `age`, `sexo` and `antiguedad` are nan\n4. Drop records belongning to above ids","metadata":{}},{"cell_type":"code","source":"# Generic pre processing steps\n\n# 1. Clean 'age' by handling 'NA' values and converting to numeric\ntrain_data['age'] = pd.to_numeric(train_data['age'].replace(' NA', np.nan), errors='coerce')\n\n# 2. Clean 'antiguedad' by handling 'NA', stripping spaces, and converting to numeric\ntrain_data['antiguedad'] = pd.to_numeric(train_data['antiguedad'].replace('     NA', np.nan).astype(str).str.strip(), errors='coerce')\n\n# 3. Select customer IDs with all 5 key fields missing (renta, nomprov, age, sexo, antiguedad)\nids_with_nan =  train_data[train_data['renta'].isna() & \n                train_data['nomprov'].isna() & \n                train_data['age'].isna() &\n                train_data['sexo'].isna() & \n                train_data['antiguedad'].isna()]['ncodpers']\n\nprint(f\"Total customers with no age, renta, sexo, antiguedad and nomprov record : {len(set(ids_with_nan))}\")\n\ndf_train = train_data[~(train_data['age'].isna() & train_data['nomprov'].isna() & train_data['renta'].isna() & train_data['sexo'].isna() & train_data['antiguedad'].isna())]\nprint(f\"Shape of data after removing customers with no age, nomprov, renta, sexo and antiguedad : {df_train.shape}\")\n\n# Sanity Check - Number of records after dropping should be equal to total records - len(ids_with_nan)\nassert df_train.shape[0] == (train_data.shape[0] - len(ids_with_nan))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculating 33rd and 67th percentile of renta for bucketing","metadata":{}},{"cell_type":"code","source":"q_33 = df_train['renta'].quantile(0.33)\nprint(f\"33rd percentile of renta : {q_33}\")\nq_67 = df_train['renta'].quantile(0.67)\nprint(f\"67th percentile of renta : {q_67}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to bucketize age into ranges\ndef age_bucket(age):\n    if pd.isna(age):          # If age is missing, return \"UNK\"\n        return \"UNK\"\n    elif age < 18:            # Below 18\n        return \"0-17\"\n    elif age < 30:            # 18 to 29\n        return \"18-29\"\n    elif age < 45:            # 30 to 44\n        return \"30-44\"\n    elif age < 60:            # 45 to 59\n        return \"45-59\"\n    else:                     # 60 and above\n        return \"60+\"\n\n# Function to bucketize income into categories\ndef income_bucket(income):\n    if pd.isna(income):       # If income is missing\n        return \"UNK\"\n    elif income <= 80000:     # Low income\n        return \"L\"\n    elif income <= 133700:    # Medium income\n        return \"M\"\n    else:                     # High income\n        return \"H\"\n\n# Function to bucketize location (province)\ndef nomprov_bucket(loc):\n    if pd.isna(loc):          # If location is missing\n        return \"UNK\"\n    else:                     # Otherwise, return the location itself\n        return loc\n\n# Function to bucketize gender\ndef sexo_bucket(gdr):\n    if pd.isna(gdr):          # If gender is missing\n        return \"UNK\"\n    else:                     # Otherwise, return gender itself\n        return gdr\n\n# Function to bucketize customer segment\ndef segmento_bucket(seg):\n    if pd.isna(seg):          # If segment is missing\n        return \"UNK\"\n        \n    if seg == \"01 - TOP\":     # Top segment\n        return \"TOP\"\n        \n    if seg == \"02 - PARTICULARES\":  # Individual customers\n        return \"PARTICULARES\"\n        \n    if seg == \"03 - UNIVERSITARIO\": # University students\n        return \"UNIVERSITARIO\"\n\ndef antiguedad_bucket(mths):\n    if pd.isna(mths): # If months is missing\n        return \"UNK\"\n    if mths <= 60: # Less than or equal to 60 months -> Young customer\n        return \"Y\"\n    if mths > 60 and mths <= 180: # More than 60 and less than 180 months -> Mid customer\n        return \"M\"\n    if mths > 180: # More than 180 months -> Old customer\n        return \"O\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performing bucketzation and creating segments\n1. `bucket_functions` is a dict which contain features to consider for bucketing and the corresponding function to implement\n2. Apply bucketization for all features and store it in `<feature>_bucket` column\n3. Create segments in format `<age_bucket>_<nomprov_bucket>_<sexo_bucket>_<renta_bucket>_<antiguedad_bucket>_<segmento_bucket>`","metadata":{}},{"cell_type":"code","source":"# Mapping of columns to their bucketization functions\nbucket_functions = {\n    'age': age_bucket,\n    'renta': income_bucket,\n    'nomprov': nomprov_bucket,\n    'segmento': segmento_bucket,\n    'sexo': sexo_bucket,          # fixed here\n    'antiguedad': antiguedad_bucket\n}\n\n# Apply bucketization dynamically\nfor col, func in bucket_functions.items():\n    df_train[f'{col}_bucket'] = df_train[col].apply(func)\n\n\ndf_train['segment'] = (\n    df_train['age_bucket'] + \"_\" +\n    df_train['nomprov_bucket'].astype(str) + \"_\" +\n    df_train['sexo_bucket'].astype(str) + \"_\" +\n    df_train['renta_bucket'] + \"_\" +\n    df_train['antiguedad_bucket'] + \"_\" +\n    df_train['segmento_bucket'].astype(str)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting dataset into train and eval\n\n\n1. The cutoff date (`training_cutoff`) for splitting is set as 2016-04-28\n2. Data before `training_cutoff` is taken as training data and data after this is eval data","metadata":{}},{"cell_type":"code","source":"# Splitting into train and eval set. The last month's data is kept as evaluation dataset\n\ntraining_cutoff = df_train['fecha_dato'].sort_values().unique()[-2]\n\ntrain_df = df_train[df_train['fecha_dato'] <= training_cutoff]\neval_df = df_train[df_train['fecha_dato'] > training_cutoff]\n\nprint(f\"Cutoff date : {training_cutoff}\")\nprint(f\"Data before {training_cutoff} is for training and data after is for evaluation\")\nprint(f\"Train data shape : {train_df.shape}\")\nprint(f\"Eval data shape : {eval_df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding most popular products\n\n1. For all the customers, find the product which have been ever owned (1 if owned atleast once, else 0) (`ever_owned`)\n2. Get the corresponding segment from `df_train` and merge both dataframes\n3. Group the merged dataframe `ever_owned` based on `segment` and find top products for each segment","metadata":{}},{"cell_type":"code","source":"product_cols = get_all_products()\never_owned = train_df.groupby('ncodpers')[product_cols].max()\nsegment_info = df_train[['ncodpers', 'segment']].drop_duplicates(subset='ncodpers').set_index('ncodpers')\never_owned = ever_owned.merge(segment_info, left_index=True, right_index=True)\n\nsegment_popularity = ever_owned.groupby('segment')[product_cols].sum()\n\n# Storing the top products for each segment in segment_top_products\nsegment_top_products = {\n    segment: segment_popularity.loc[segment].sort_values(ascending=False).index.tolist()\n    for segment in segment_popularity.index\n}\n\nprint(f\"Total segments : {len(segment_popularity)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top Products Across All Customers\n\n* `top_products` stores the most frequently owned products across the entire data\n* Acts as a fallback recommendation list for cases where a customer ID is missing or not found during evaluation","metadata":{}},{"cell_type":"code","source":"columns_to_use = ['ncodpers'] + product_cols\ntrain_df_gbl = train_df[columns_to_use]\never_owned_products = train_df_gbl.groupby('ncodpers')[product_cols].max()\nproduct_popularity = ever_owned_products.sum(axis=0).sort_values(ascending=False)\ntop_products = product_popularity.index.tolist()\ndel train_df_gbl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Recommending on evaluation data\n- `baseline3_recommend_for_user(..)` - Takes the user id and returns top product recommendations which have not been owned by user.","metadata":{}},{"cell_type":"code","source":"def baseline3_recommend_for_eval_user(user_id, top_n=7):\n    # If user id is not present, recommend the top 7 found by considering all users\n    if user_id not in ever_owned.index:\n        return top_products[:top_n]\n    # If user id is present, find products owned by the user and it's segment and suggest top products in that segment never owned by the user       \n    user_data = ever_owned.loc[user_id]\n    user_segment = user_data['segment']\n    owned_products = set(user_data[product_cols][user_data[product_cols] == 1].index)\n    popular_products = segment_top_products.get(user_segment, [])\n    recs = [p for p in popular_products if p not in owned_products]\n    return recs[:top_n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation Predictions","metadata":{}},{"cell_type":"code","source":"# Eval predictions\n\n# Get unique customers in May 2016\neval_users = eval_df['ncodpers'].unique()\nprint(f\"total users in eval set: {len(eval_users)}\")\n\nbaseline_eval_preds = pd.DataFrame({\n    \"ncodpers\": eval_users\n})\n\nbaseline_eval_preds[\"added_products\"] = baseline_eval_preds[\"ncodpers\"].map(lambda uid: baseline3_recommend_for_eval_user(uid, top_n=7))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ground truth for evaluation predictions","metadata":{}},{"cell_type":"code","source":"# Ground truth from eval_df\neval_actual = []\nfor _, row in eval_df.groupby('ncodpers'):\n    added = [prod for prod in product_cols if row[prod].values[0] == 1]\n    eval_actual.append(added)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Computing MAP@7 for baseline evaluation data","metadata":{}},{"cell_type":"code","source":"map_score_eval = mapk(eval_actual, baseline_eval_preds['added_products'].tolist())\nprint(f\"Baseline 3 MAP@7 (Eval data): {map_score_eval:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Error Analysis\n\n- We have evaluation ground truth data `eval_actual` and evaluation predictions `eval_preds`\n- Using these data, we will compute the **False Positives** and **False Negatives**","metadata":{}},{"cell_type":"code","source":"fp_counter,fn_counter = Counter(), Counter()\n\neval_preds = baseline_eval_preds['added_products'].tolist()\n\nfor true_prod, pred_prod in zip(eval_actual, eval_preds):\n    true_set = set(true_prod)\n    pred_set = set(pred_prod)\n\n    # False Negatives: missed actual buys\n    fn = true_set - pred_set\n    fn_counter.update(fn)\n\n    # False Positives: predicted but not actually bought\n    fp = pred_set - true_set\n    fp_counter.update(fp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top 10 False Negatives","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 false negatives (bought but not predicted):\")\nfor product, count in fn_counter.most_common(10):\n    print(f\"{product} : {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top 10 False Positives","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 false positives (predicted but not bought):\")\nfor product, count in fp_counter.most_common(10):\n    print(f\"{product} : {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Recommendation on the Test Set","metadata":{}},{"cell_type":"code","source":"# Define recommendation function per user\never_owned = df_train.groupby('ncodpers')[product_cols].max()\nsegment_info = df_train[['ncodpers', 'segment']].drop_duplicates(subset='ncodpers').set_index('ncodpers')\never_owned = ever_owned.merge(segment_info, left_index=True, right_index=True)\n\n\ndef recommend_for_user(user_id, top_n=7):\n    if user_id not in ever_owned.index:\n        print(f\"User not found: {user_id}\")\n        return []\n    user_data = ever_owned.loc[user_id]\n    user_segment = user_data['segment']\n    owned_products = set(user_data[product_cols][user_data[product_cols] == 1].index)\n    popular_products = segment_top_products.get(user_segment, [])\n    recs = [p for p in popular_products if p not in owned_products]\n    return recs[:top_n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Batch Prediction\n\n- Implemented to prevent kernel crash","metadata":{}},{"cell_type":"code","source":"batch_size = 400000  \nn = len(sample_data)\nall_recs = []\n\nfor start in range(0, n, batch_size):\n    end = start + batch_size\n    print(f\"Recommending for {start} to {end}\")\n    batch = sample_data.iloc[start:end]\n    recs = batch['ncodpers'].progress_apply(lambda uid: \" \".join(recommend_for_user(uid, top_n=7)))\n    all_recs.extend(recs)\n\nsample_data['added_products'] = all_recs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_data.to_csv(\"Baseline3_test_preds.csv\",index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Seasonality","metadata":{}},{"cell_type":"markdown","source":"## Collaborative filtering - Exploration","metadata":{}},{"cell_type":"code","source":"# from scipy.sparse import csr_matrix\n# from sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_set = train_set[product_cols].fillna(0)\n# product_mtx = csr_matrix(train_set[product_cols].values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"product_cols = get_all_products()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:09:52.130687Z","iopub.execute_input":"2025-08-19T03:09:52.130953Z","iopub.status.idle":"2025-08-19T03:09:52.135051Z","shell.execute_reply.started":"2025-08-19T03:09:52.130932Z","shell.execute_reply":"2025-08-19T03:09:52.134328Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data = train_data[train_data['fecha_dato'] == \"2016-04-28\"]\ndata.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:11:12.096507Z","iopub.execute_input":"2025-08-19T03:11:12.096786Z","iopub.status.idle":"2025-08-19T03:11:13.152481Z","shell.execute_reply.started":"2025-08-19T03:11:12.096768Z","shell.execute_reply":"2025-08-19T03:11:13.151897Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(928274, 48)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n    \nuser_item_matrix = data[product_cols]\n\n# Compute item-item similarity (24 x 24)\nitem_sim = cosine_similarity(user_item_matrix.T)\nitem_sim_df = pd.DataFrame(item_sim, index=product_cols, columns=product_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:14:01.674550Z","iopub.execute_input":"2025-08-19T03:14:01.675243Z","iopub.status.idle":"2025-08-19T03:14:02.482636Z","shell.execute_reply.started":"2025-08-19T03:14:01.675219Z","shell.execute_reply":"2025-08-19T03:14:02.481836Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"user_products = data[data['ncodpers']==896849][product_cols].iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:28:47.369808Z","iopub.execute_input":"2025-08-19T03:28:47.370087Z","iopub.status.idle":"2025-08-19T03:28:47.376025Z","shell.execute_reply.started":"2025-08-19T03:28:47.370066Z","shell.execute_reply":"2025-08-19T03:28:47.375455Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#user_products\nscores = item_sim_df.dot(user_products)\n#scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:29:43.625482Z","iopub.execute_input":"2025-08-19T03:29:43.625782Z","iopub.status.idle":"2025-08-19T03:29:43.629796Z","shell.execute_reply.started":"2025-08-19T03:29:43.625759Z","shell.execute_reply":"2025-08-19T03:29:43.629105Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"scores = scores * (1 - user_products)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:29:51.295185Z","iopub.execute_input":"2025-08-19T03:29:51.295457Z","iopub.status.idle":"2025-08-19T03:29:51.299491Z","shell.execute_reply.started":"2025-08-19T03:29:51.295437Z","shell.execute_reply":"2025-08-19T03:29:51.298928Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"scores.sort_values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T03:30:01.786361Z","iopub.execute_input":"2025-08-19T03:30:01.787113Z","iopub.status.idle":"2025-08-19T03:30:01.793251Z","shell.execute_reply.started":"2025-08-19T03:30:01.787077Z","shell.execute_reply":"2025-08-19T03:30:01.792700Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"ind_ctma_fin_ult1    0.000000\nind_cno_fin_ult1     0.000000\nind_ecue_fin_ult1    0.000000\nind_tjcr_fin_ult1    0.000000\nind_ctju_fin_ult1    0.000042\nind_ahor_fin_ult1    0.011080\nind_aval_fin_ult1    0.032449\nind_deco_fin_ult1    0.040403\nind_cder_fin_ult1    0.041170\nind_pres_fin_ult1    0.044861\nind_deme_fin_ult1    0.065579\nind_viv_fin_ult1     0.111155\nind_ctop_fin_ult1    0.336907\nind_plan_fin_ult1    0.368979\nind_hip_fin_ult1     0.371715\nind_cco_fin_ult1     0.409931\nind_fond_fin_ult1    0.416722\nind_ctpp_fin_ult1    0.527205\nind_valo_fin_ult1    0.532790\nind_dela_fin_ult1    0.537754\nind_reca_fin_ult1    0.942148\nind_recibo_ult1      1.387193\nind_nomina_ult1      1.462698\nind_nom_pens_ult1    1.535896\ndtype: float64"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Compute scores by multiplying similarity with user’s owned products\nscores = item_sim_df.dot(user_products)\n\n# Remove already owned products\nscores = scores * (1 - user_products)\n\n# Sort and get top-N\nreturn scores.sort_values(ascending=False).head(top_n).index.tolist()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}